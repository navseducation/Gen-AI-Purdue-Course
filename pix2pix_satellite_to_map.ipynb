{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navseducation/Gen-AI-Purdue-Course/blob/main/pix2pix_satellite_to_map.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6311d7a8",
      "metadata": {
        "id": "6311d7a8"
      },
      "source": [
        "\n",
        "# Pix2Pix: Satellite → Map Translation (PyTorch)\n",
        "\n",
        "This notebook trains a **Pix2Pix** model to translate **satellite images → map renderings** using the original **maps** dataset from the Pix2Pix paper.\n",
        "\n",
        "**What you'll learn**\n",
        "- How a **U-Net generator** + **PatchGAN discriminator** work\n",
        "- How to balance **GAN loss** with **L1 reconstruction loss**\n",
        "- Tips for stability, visualization, and evaluation\n",
        "\n",
        "> Run this in a GPU runtime for best results. The dataset is ~255MB.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6a6f84a",
      "metadata": {
        "id": "f6a6f84a"
      },
      "source": [
        "## 1) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fff41539",
      "metadata": {
        "id": "fff41539"
      },
      "outputs": [],
      "source": [
        "\n",
        "# If needed, install deps in your environment (uncomment below).\n",
        "# !pip -q install torch torchvision pillow matplotlib tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d410bdfc",
      "metadata": {
        "id": "d410bdfc"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, random, math, time, glob, tarfile, pathlib\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import make_grid, save_image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "set_seed(42)\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "695a641d",
      "metadata": {
        "id": "695a641d"
      },
      "source": [
        "## 2) Download the **maps** dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de52116",
      "metadata": {
        "id": "6de52116"
      },
      "outputs": [],
      "source": [
        "\n",
        "import urllib.request\n",
        "\n",
        "root = \"./data/maps\"\n",
        "os.makedirs(root, exist_ok=True)\n",
        "archive_path = os.path.join(root, \"maps.tar.gz\")\n",
        "\n",
        "if not (os.path.exists(os.path.join(root, \"train\")) and os.path.exists(os.path.join(root, \"val\"))):\n",
        "    url = \"http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/maps.tar.gz\"\n",
        "    print(\"Downloading:\", url)\n",
        "    urllib.request.urlretrieve(url, archive_path)\n",
        "    print(\"Extracting...\")\n",
        "    with tarfile.open(archive_path, \"r:gz\") as tar:\n",
        "        tar.extractall(path=\"./data\")\n",
        "    print(\"Done.\")\n",
        "else:\n",
        "    print(\"Dataset already present.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "502163fd",
      "metadata": {
        "id": "502163fd"
      },
      "source": [
        "\n",
        "## 3) Dataset Loader (paired images)\n",
        "Each file is a **side-by-side pair**: left half is one domain, right half is the other.  \n",
        "For the **maps** dataset: **left = aerial (satellite)**, **right = map**.\n",
        "\n",
        "If your preview looks flipped, set `flip_pair=True` below to swap halves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab3c790",
      "metadata": {
        "id": "5ab3c790"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PairedImageDataset(Dataset):\n",
        "    def __init__(self, folder, image_size=256, flip_pair=False, augment=True):\n",
        "        self.paths = sorted(glob.glob(os.path.join(folder, \"*.jpg\")))\n",
        "        self.size = image_size\n",
        "        self.flip_pair = flip_pair\n",
        "        self.augment = augment\n",
        "\n",
        "        self.to_tensor = T.ToTensor()\n",
        "        self.normalize = T.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
        "        self.resize = T.Resize((self.size, self.size), interpolation=T.InterpolationMode.BICUBIC)\n",
        "\n",
        "    def __len__(self): return len(self.paths)\n",
        "\n",
        "    def _split_pair(self, img):\n",
        "        w, h = img.size\n",
        "        w2 = w // 2\n",
        "        left = img.crop((0, 0, w2, h))\n",
        "        right = img.crop((w2, 0, w, h))\n",
        "        if self.flip_pair:\n",
        "            left, right = right, left\n",
        "        return left, right\n",
        "\n",
        "    def _augment_pair(self, a, b):\n",
        "        # Random horizontal flip keeps correspondence\n",
        "        if self.augment and random.random() < 0.5:\n",
        "            a = a.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            b = b.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        return a, b\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        a, b = self._split_pair(img)           # a: satellite, b: map (default for maps dataset)\n",
        "        a, b = self._augment_pair(a, b)\n",
        "        a = self.normalize(self.to_tensor(self.resize(a)))\n",
        "        b = self.normalize(self.to_tensor(self.resize(b)))\n",
        "        return a, b  # (input, target)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53bcf4b7",
      "metadata": {
        "id": "53bcf4b7"
      },
      "source": [
        "### Preview a few pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "270092ea",
      "metadata": {
        "id": "270092ea"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dir = \"./data/maps/train\"\n",
        "val_dir = \"./data/maps/val\"\n",
        "\n",
        "train_ds = PairedImageDataset(train_dir, image_size=256, flip_pair=False, augment=True)\n",
        "val_ds = PairedImageDataset(val_dir, image_size=256, flip_pair=False, augment=False)\n",
        "\n",
        "def denorm(x):\n",
        "    return (x * 0.5 + 0.5).clamp(0,1)\n",
        "\n",
        "def show_batch(ds, n=4):\n",
        "    idxs = random.sample(range(len(ds)), n)\n",
        "    rows = []\n",
        "    for i in idxs:\n",
        "        a, b = ds[i]\n",
        "        rows.append(torch.stack([a, b]))\n",
        "    grid = make_grid(torch.cat(rows, dim=0), nrow=2)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(denorm(grid).permute(1,2,0))\n",
        "    plt.show()\n",
        "\n",
        "show_batch(train_ds, n=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b4021cf",
      "metadata": {
        "id": "9b4021cf"
      },
      "source": [
        "## 4) Models — U-Net Generator & PatchGAN Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "172fa26e",
      "metadata": {
        "id": "172fa26e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Building blocks ---\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_c, out_c, norm=True):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1, bias=not norm)]\n",
        "        if norm: layers += [nn.BatchNorm2d(out_c)]\n",
        "        layers += [nn.LeakyReLU(0.2, inplace=True)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_c, out_c, dropout=False):\n",
        "        super().__init__()\n",
        "        layers = [nn.ConvTranspose2d(in_c, out_c, 4, 2, 1, bias=False),\n",
        "                  nn.BatchNorm2d(out_c),\n",
        "                  nn.ReLU(True)]\n",
        "        if dropout: layers += [nn.Dropout(0.5)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class UNetGenerator(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=3, features=64):\n",
        "        super().__init__()\n",
        "        # Encoder\n",
        "        self.d1 = Down(in_ch, features, norm=False)   # 128\n",
        "        self.d2 = Down(features, features*2)          # 64\n",
        "        self.d3 = Down(features*2, features*4)        # 32\n",
        "        self.d4 = Down(features*4, features*8)        # 16\n",
        "        self.d5 = Down(features*8, features*8)        # 8\n",
        "        self.d6 = Down(features*8, features*8)        # 4\n",
        "        self.d7 = Down(features*8, features*8)        # 2\n",
        "        self.d8 = Down(features*8, features*8, norm=False) # 1\n",
        "\n",
        "        # Decoder\n",
        "        self.u1 = Up(features*8, features*8, dropout=True)\n",
        "        self.u2 = Up(features*16, features*8, dropout=True)\n",
        "        self.u3 = Up(features*16, features*8, dropout=True)\n",
        "        self.u4 = Up(features*16, features*8)\n",
        "        self.u5 = Up(features*16, features*4)\n",
        "        self.u6 = Up(features*8, features*2)\n",
        "        self.u7 = Up(features*4, features)\n",
        "        self.outc = nn.Sequential(\n",
        "            nn.ConvTranspose2d(features*2, out_ch, 4, 2, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        d1 = self.d1(x); d2 = self.d2(d1); d3 = self.d3(d2); d4 = self.d4(d3)\n",
        "        d5 = self.d5(d4); d6 = self.d6(d5); d7 = self.d7(d6); bottleneck = self.d8(d7)\n",
        "        u1 = self.u1(bottleneck)\n",
        "        u2 = self.u2(torch.cat([u1, d7], dim=1))\n",
        "        u3 = self.u3(torch.cat([u2, d6], dim=1))\n",
        "        u4 = self.u4(torch.cat([u3, d5], dim=1))\n",
        "        u5 = self.u5(torch.cat([u4, d4], dim=1))\n",
        "        u6 = self.u6(torch.cat([u5, d3], dim=1))\n",
        "        u7 = self.u7(torch.cat([u6, d2], dim=1))\n",
        "        out = self.outc(torch.cat([u7, d1], dim=1))\n",
        "        return out\n",
        "\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self, in_ch=3, cond_ch=3, features=64):\n",
        "        super().__init__()\n",
        "        # Input is concatenation of condition (satellite) and output/target (map)\n",
        "        ch = in_ch + cond_ch\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(ch, features, 4, 2, 1), nn.LeakyReLU(0.2, inplace=True),        # 128\n",
        "            nn.Conv2d(features, features*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*2), nn.LeakyReLU(0.2, inplace=True),              # 64\n",
        "            nn.Conv2d(features*2, features*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*4), nn.LeakyReLU(0.2, inplace=True),              # 32\n",
        "            nn.Conv2d(features*4, features*8, 4, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(features*8), nn.LeakyReLU(0.2, inplace=True),              # 31x31\n",
        "            nn.Conv2d(features*8, 1, 4, 1, 1)  # Patch logits\n",
        "        )\n",
        "    def forward(self, x, cond):\n",
        "        return self.net(torch.cat([x, cond], dim=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30ddf10d",
      "metadata": {
        "id": "30ddf10d"
      },
      "source": [
        "## 5) Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f132bc0",
      "metadata": {
        "id": "1f132bc0"
      },
      "outputs": [],
      "source": [
        "\n",
        "G = UNetGenerator().to(device)\n",
        "D = PatchDiscriminator().to(device)\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "        try:\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        except Exception:\n",
        "            pass\n",
        "        if hasattr(m, \"bias\") and m.bias is not None:\n",
        "            nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "G.apply(init_weights); D.apply(init_weights);\n",
        "\n",
        "lr = 2e-4\n",
        "beta1, beta2 = 0.5, 0.999\n",
        "lambda_L1 = 100.0\n",
        "epochs = 40\n",
        "batch_size = 8\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False)\n",
        "\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "l1 = nn.L1Loss()\n",
        "opt_G = optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "opt_D = optim.Adam(D.parameters(), lr=lr, betas=(beta1, beta2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c0ffe82",
      "metadata": {
        "id": "2c0ffe82"
      },
      "source": [
        "## 6) Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dcfc973",
      "metadata": {
        "id": "5dcfc973"
      },
      "outputs": [],
      "source": [
        "\n",
        "def sample_and_show(G, ds, n=4, title='Samples'):\n",
        "    G.eval()\n",
        "    with torch.no_grad():\n",
        "        idxs = random.sample(range(len(ds)), n)\n",
        "        fakes = []\n",
        "        rows = []\n",
        "        for i in idxs:\n",
        "            a, b = ds[i]\n",
        "            a_in = a.unsqueeze(0).to(device)\n",
        "            fake = G(a_in).cpu().squeeze(0)\n",
        "            rows.append(torch.stack([a, fake, b]))\n",
        "        grid = make_grid(torch.cat(rows, dim=0), nrow=3)\n",
        "        plt.figure(figsize=(9, 9))\n",
        "        plt.axis('off')\n",
        "        plt.title(title)\n",
        "        plt.imshow((grid*0.5+0.5).clamp(0,1).permute(1,2,0))\n",
        "        plt.show()\n",
        "    G.train()\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    G.train(); D.train()\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
        "    for a, b in pbar:  # a: satellite (cond), b: map (target)\n",
        "        a, b = a.to(device), b.to(device)\n",
        "        # --- Train Discriminator ---\n",
        "        with torch.no_grad():\n",
        "            fake_b = G(a)\n",
        "        real_logits = D(b, a)\n",
        "        fake_logits = D(fake_b.detach(), a)\n",
        "        loss_D = bce(real_logits, torch.ones_like(real_logits)) +                  bce(fake_logits, torch.zeros_like(fake_logits))\n",
        "        opt_D.zero_grad(); loss_D.backward(); opt_D.step()\n",
        "\n",
        "        # --- Train Generator ---\n",
        "        fake_b = G(a)\n",
        "        fake_logits = D(fake_b, a)\n",
        "        loss_G_adv = bce(fake_logits, torch.ones_like(fake_logits))\n",
        "        loss_G_l1 = l1(fake_b, b) * lambda_L1\n",
        "        loss_G = loss_G_adv + loss_G_l1\n",
        "        opt_G.zero_grad(); loss_G.backward(); opt_G.step()\n",
        "\n",
        "        pbar.set_postfix(loss_D=float(loss_D.item()), loss_G=float(loss_G.item()))\n",
        "\n",
        "    # Show progress each epoch\n",
        "    sample_and_show(G, val_ds, n=4, title=f\"Epoch {epoch}\")\n",
        "\n",
        "# Save models\n",
        "torch.save(G.state_dict(), \"pix2pix_sat2map_G.pt\")\n",
        "torch.save(D.state_dict(), \"pix2pix_sat2map_D.pt\")\n",
        "print(\"Saved pix2pix models.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4645711f",
      "metadata": {
        "id": "4645711f"
      },
      "source": [
        "\n",
        "## 7) Tips, Troubleshooting, and Extensions\n",
        "\n",
        "- **If outputs look flipped**: set `flip_pair=True` in `PairedImageDataset`.\n",
        "- **Stability**:\n",
        "  - Reduce learning rate to `1e-4` if training oscillates.\n",
        "  - Add instance noise (tiny Gaussian) to inputs of D.\n",
        "  - Try one-sided label smoothing for real labels.\n",
        "- **Higher resolution**: train on 512×512 tiles (requires more VRAM).\n",
        "- **Metrics**: Compute FID on held-out tiles for objective comparison.\n",
        "- **Other domains**: Swap dataset for your own paired tiles from QGIS/ArcGIS.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}